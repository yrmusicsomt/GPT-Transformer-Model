{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *Build a tiny transformer model from scratch*\n",
        "\n"
      ],
      "metadata": {
        "id": "kPPrT9ZCERLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Setup and Import Necessary Libraries**"
      ],
      "metadata": {
        "id": "PtdMpoyGJPAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rHAARjHWEPXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n"
      ],
      "metadata": {
        "id": "lc3Hcqw_b3U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Define Positional Encoding**"
      ],
      "metadata": {
        "id": "MrR0xpQRJRHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n"
      ],
      "metadata": {
        "id": "WPb_hgSvGf5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Define Multi-Head Self-Attention**"
      ],
      "metadata": {
        "id": "iL2YENiGJfp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert embedding_dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embedding_dim // num_heads\n",
        "\n",
        "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.fc_out = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, values, keys, query, mask=None):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.num_heads different pieces\n",
        "        values = values.reshape(N, value_len, self.num_heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.head_dim ** (1 / 2)), dim=3)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.head_dim * self.num_heads\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "01UrORlWGvJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Define Feedforward Neural Network**"
      ],
      "metadata": {
        "id": "Ynj6K73NJu7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embedding_dim, forward_expansion):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, forward_expansion * embedding_dim)\n",
        "        self.fc2 = nn.Linear(forward_expansion * embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "T99ZmEDMG2vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Define the Transformer Block**"
      ],
      "metadata": {
        "id": "vHBY1cUyJzuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, forward_expansion, dropout):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(embedding_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "        self.feed_forward = FeedForward(embedding_dim, forward_expansion)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        # Add skip connection, run through normalization and finally dropout\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "IvWW5b9eG9XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Define the Full Transformer Model**"
      ],
      "metadata": {
        "id": "FqRDjSdHKuwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,\n",
        "                 embedding_dim=512, num_heads=8, num_layers=6,\n",
        "                 forward_expansion=4, dropout=0, max_length=100):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_dim)\n",
        "        self.src_position_embedding = PositionalEncoding(embedding_dim, max_length)\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_dim)\n",
        "        self.trg_position_embedding = PositionalEncoding(embedding_dim, max_length)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embedding_dim, num_heads, forward_expansion, dropout)\n",
        "             for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(embedding_dim, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            N, 1, trg_len, trg_len\n",
        "        )\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        src_embedding = self.dropout(self.src_word_embedding(src) * math.sqrt(src.size(1)))\n",
        "        src_embedding = self.src_position_embedding(src_embedding)\n",
        "        trg_embedding = self.dropout(self.trg_word_embedding(trg) * math.sqrt(trg.size(1)))\n",
        "        trg_embedding = self.trg_position_embedding(trg_embedding)\n",
        "\n",
        "        out = src_embedding\n",
        "        for layer in self.transformer_blocks:\n",
        "            out = layer(out, out, out, src_mask)\n",
        "\n",
        "        out = trg_embedding\n",
        "        for layer in self.transformer_blocks:\n",
        "            out = layer(out, out, out, trg_mask)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "X86x9lMWHY9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Define a Small Example for Training (with Sample Sentences)**"
      ],
      "metadata": {
        "id": "xGaVTj4qLKvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentences for training\n",
        "sentences = [\n",
        "    \"the cat sat on the mat\",\n",
        "    \"the dog chased the cat\",\n",
        "    \"the bird sang a song\",\n",
        "    \"a fish swims in the sea\",\n",
        "    \"the sun rises in the east\"\n",
        "]\n",
        "\n",
        "# Create a character-to-ID and ID-to-character mappings\n",
        "chars = sorted(list(set(\"\".join(sentences))))\n",
        "char_to_id = {char: idx for idx, char in enumerate(chars)}\n",
        "id_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "\n",
        "# Encode sentences into sequences of IDs\n",
        "encoded_sentences = [[char_to_id[char] for char in sentence] for sentence in sentences]\n",
        "\n",
        "# Convert sequences into tensor format\n",
        "max_len = max(len(sentence) for sentence in encoded_sentences)\n",
        "padded_sentences = [sentence + [char_to_id[\" \"]] * (max_len - len(sentence)) for sentence in encoded_sentences]\n",
        "input_data = torch.tensor(padded_sentences, dtype=torch.long)\n",
        "\n",
        "# Targets will be the same sentences shifted by one character (teacher forcing)\n",
        "target_data = torch.tensor([sentence[1:] + [char_to_id[\" \"]] for sentence in padded_sentences], dtype=torch.long)\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 32\n",
        "num_heads = 2\n",
        "num_layers = 2\n",
        "forward_expansion = 4\n",
        "dropout = 0.1\n",
        "max_length = max_len\n",
        "src_vocab_size = len(chars)\n",
        "trg_vocab_size = len(chars)\n",
        "src_pad_idx = char_to_id[\" \"]\n",
        "trg_pad_idx = char_to_id[\" \"]\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Model initialization\n",
        "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,\n",
        "                    embedding_dim, num_heads, num_layers,\n",
        "                    forward_expansion, dropout, max_length)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
        "\n",
        "# Adjusted training loop to ensure correct dimensions\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(input_data, input_data)\n",
        "\n",
        "    # Reshape output: (batch_size, seq_len - 1, vocab_size) -> (batch_size * (seq_len - 1), vocab_size)\n",
        "    output = output[:, :-1, :].reshape(-1, output.shape[-1])\n",
        "\n",
        "    # Reshape target_data: (batch_size, seq_len - 1) -> (batch_size * (seq_len - 1))\n",
        "    target = target_data[:, 1:].reshape(-1)\n",
        "\n",
        "    # Ensure that the output and target are aligned correctly\n",
        "    assert output.size(0) == target.size(0), \"Output and target sizes do not match\"\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jzEhlVxJHkbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d858b582-9435-427c-9921-46a4a3d41a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 2.5069\n",
            "Epoch [20/1000], Loss: 2.2377\n",
            "Epoch [30/1000], Loss: 2.1104\n",
            "Epoch [40/1000], Loss: 1.9969\n",
            "Epoch [50/1000], Loss: 1.8672\n",
            "Epoch [60/1000], Loss: 1.6969\n",
            "Epoch [70/1000], Loss: 1.7683\n",
            "Epoch [80/1000], Loss: 1.6004\n",
            "Epoch [90/1000], Loss: 1.5333\n",
            "Epoch [100/1000], Loss: 1.5024\n",
            "Epoch [110/1000], Loss: 1.4961\n",
            "Epoch [120/1000], Loss: 1.3856\n",
            "Epoch [130/1000], Loss: 1.3198\n",
            "Epoch [140/1000], Loss: 1.4195\n",
            "Epoch [150/1000], Loss: 1.3259\n",
            "Epoch [160/1000], Loss: 1.3530\n",
            "Epoch [170/1000], Loss: 1.3315\n",
            "Epoch [180/1000], Loss: 1.3342\n",
            "Epoch [190/1000], Loss: 1.3444\n",
            "Epoch [200/1000], Loss: 1.2921\n",
            "Epoch [210/1000], Loss: 1.2615\n",
            "Epoch [220/1000], Loss: 1.3209\n",
            "Epoch [230/1000], Loss: 1.3713\n",
            "Epoch [240/1000], Loss: 1.2837\n",
            "Epoch [250/1000], Loss: 1.1927\n",
            "Epoch [260/1000], Loss: 1.2934\n",
            "Epoch [270/1000], Loss: 1.2728\n",
            "Epoch [280/1000], Loss: 1.2460\n",
            "Epoch [290/1000], Loss: 1.2093\n",
            "Epoch [300/1000], Loss: 1.1664\n",
            "Epoch [310/1000], Loss: 1.1216\n",
            "Epoch [320/1000], Loss: 1.2054\n",
            "Epoch [330/1000], Loss: 1.1929\n",
            "Epoch [340/1000], Loss: 1.1570\n",
            "Epoch [350/1000], Loss: 1.2164\n",
            "Epoch [360/1000], Loss: 1.1360\n",
            "Epoch [370/1000], Loss: 1.1643\n",
            "Epoch [380/1000], Loss: 1.0772\n",
            "Epoch [390/1000], Loss: 1.1230\n",
            "Epoch [400/1000], Loss: 1.0488\n",
            "Epoch [410/1000], Loss: 0.9500\n",
            "Epoch [420/1000], Loss: 1.0732\n",
            "Epoch [430/1000], Loss: 0.9896\n",
            "Epoch [440/1000], Loss: 0.9423\n",
            "Epoch [450/1000], Loss: 1.0659\n",
            "Epoch [460/1000], Loss: 1.0150\n",
            "Epoch [470/1000], Loss: 1.0166\n",
            "Epoch [480/1000], Loss: 0.9268\n",
            "Epoch [490/1000], Loss: 0.9200\n",
            "Epoch [500/1000], Loss: 1.0028\n",
            "Epoch [510/1000], Loss: 0.9222\n",
            "Epoch [520/1000], Loss: 0.8616\n",
            "Epoch [530/1000], Loss: 1.0073\n",
            "Epoch [540/1000], Loss: 0.9737\n",
            "Epoch [550/1000], Loss: 0.9027\n",
            "Epoch [560/1000], Loss: 0.9044\n",
            "Epoch [570/1000], Loss: 0.8933\n",
            "Epoch [580/1000], Loss: 0.8908\n",
            "Epoch [590/1000], Loss: 0.8121\n",
            "Epoch [600/1000], Loss: 0.8003\n",
            "Epoch [610/1000], Loss: 0.7364\n",
            "Epoch [620/1000], Loss: 0.9013\n",
            "Epoch [630/1000], Loss: 0.8297\n",
            "Epoch [640/1000], Loss: 0.8630\n",
            "Epoch [650/1000], Loss: 0.8338\n",
            "Epoch [660/1000], Loss: 0.6830\n",
            "Epoch [670/1000], Loss: 0.7326\n",
            "Epoch [680/1000], Loss: 0.8171\n",
            "Epoch [690/1000], Loss: 0.7706\n",
            "Epoch [700/1000], Loss: 0.7189\n",
            "Epoch [710/1000], Loss: 0.6918\n",
            "Epoch [720/1000], Loss: 0.7523\n",
            "Epoch [730/1000], Loss: 0.8055\n",
            "Epoch [740/1000], Loss: 0.7810\n",
            "Epoch [750/1000], Loss: 0.8498\n",
            "Epoch [760/1000], Loss: 0.8550\n",
            "Epoch [770/1000], Loss: 0.7021\n",
            "Epoch [780/1000], Loss: 0.7466\n",
            "Epoch [790/1000], Loss: 0.6264\n",
            "Epoch [800/1000], Loss: 0.9234\n",
            "Epoch [810/1000], Loss: 0.7918\n",
            "Epoch [820/1000], Loss: 0.8494\n",
            "Epoch [830/1000], Loss: 0.6997\n",
            "Epoch [840/1000], Loss: 0.6757\n",
            "Epoch [850/1000], Loss: 0.6741\n",
            "Epoch [860/1000], Loss: 0.6111\n",
            "Epoch [870/1000], Loss: 0.6669\n",
            "Epoch [880/1000], Loss: 0.6113\n",
            "Epoch [890/1000], Loss: 0.5277\n",
            "Epoch [900/1000], Loss: 0.6241\n",
            "Epoch [910/1000], Loss: 0.6190\n",
            "Epoch [920/1000], Loss: 0.6377\n",
            "Epoch [930/1000], Loss: 0.6278\n",
            "Epoch [940/1000], Loss: 0.5839\n",
            "Epoch [950/1000], Loss: 0.7287\n",
            "Epoch [960/1000], Loss: 0.6010\n",
            "Epoch [970/1000], Loss: 0.5009\n",
            "Epoch [980/1000], Loss: 0.5035\n",
            "Epoch [990/1000], Loss: 0.6040\n",
            "Epoch [1000/1000], Loss: 0.6460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8. Prediction (Inference)**"
      ],
      "metadata": {
        "id": "e7WZ1gRPNLoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Generate text based on the model's predictions\n",
        "def generate_text(model, start_seq, max_len=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = torch.tensor([[char_to_id[char] for char in start_seq]], dtype=torch.long)\n",
        "        for _ in range(max_len - len(start_seq)):\n",
        "            output = model(input_ids, input_ids)\n",
        "            next_char_id = torch.argmax(output[0, -1, :]).item()\n",
        "            input_ids = torch.cat([input_ids, torch.tensor([[next_char_id]], dtype=torch.long)], dim=1)\n",
        "            if id_to_char[next_char_id] == \" \":\n",
        "                break\n",
        "        return \"\".join([id_to_char[id] for id in input_ids[0].tolist()])\n",
        "\n",
        "# Generate a text starting with \"the\"\n",
        "generated_text = generate_text(model, \"the dog \")\n",
        "print(f\"Generated text: {generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d9Pa7ZENKmf",
        "outputId": "d3da6d52-f761-4821-8709-22676018ece3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: the dog hsdtecaectec\n"
          ]
        }
      ]
    }
  ]
}